### Types of Harm in Automated-Decision Making Systems

**Allocative Harm**  
When opportunities are withheld from certain people or groups  
E.g: Racial bias in loan-worthiness systems lead to race-based loan denial.[^1]

**Representational Harm**  
When certain people or groups are stigmatized or stereotyped18  
E.g: LLMs disproportionately feature marginalized groups as victims in stories.

**Quality of Service Harms**  
When the quality of service an individual or group receives is diminished due to bias in AI systems[^2]  
E.g: People with facial differences have to wait longer for manual processing at airport security checkpoints.

**Interpersonal and Social System Harms**  
When an individual experiences interpersonal harm or societal harm due to bias in AI systems  
E.g: False arrests due to erroneous facial recognition make it difficult for people to clear their names and cause emotional injury to children traumatized by watching their parents getting arrested.[^3]

**If we fail to mitigate bias, we will build system that generate injustice.** For an overview of justice in automated-decision making systems, refer to the \[drawer:Justice Primer\](justice).

[^1]:  Suresh, *A Framework for Understanding Sources of Harm*

[^2]:  *Facial Recognition And The Facial Difference Community*

[^3]:   MacMillan, *Police Ignore Standards*